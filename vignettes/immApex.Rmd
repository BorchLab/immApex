---
title: Machine and Deep Learning Models with immApex
author: 
- name: Nick Borcherding
  email: ncborch@gmail.com
  affiliation: Washington University in St. Louis, School of Medicine, St. Louis, MO, USA
date: 'Compiled: `r format(Sys.Date(), "%B %d, %Y")`'
output:
  BiocStyle::html_document:
    toc_float: true
package: immApex
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{Machine and Deep Learning Models with immApex}
  %\VignetteEncoding{UTF-8} 
---


```{r, echo=FALSE, results="hide", message=FALSE}
knitr::opts_chunk$set(error=FALSE, message=FALSE, warning=FALSE)
library(BiocStyle)
set.seed(42)
```

# Introduction

Single-cell sequencing is an emerging technology in the field of immunology and oncology that allows researchers to couple RNA quantification and other modalities, like immune cell receptor profiling at the level of an individual cell. A number of workflows and software packages have been created to process and analyze single-cell transcriptomic data. These packages allow users to take the vast dimensionality of the data generated in single-cell-based experiments and distill the data into novel insights. Some of the packages within the R environment that offer single-cell immune profiling support include [scRepertoire](https://github.com/BorchLab/scRepertoire), [immunarch](https://github.com/immunomind/immunarch), and [immcantation](https://immcantation.readthedocs.io/en/stable/). None of these packages offer support for machine or deep learning applications for immune repertoire profiling. 

**immApex** is meant to serve as an API for deep-learning models based on immune receptor sequencing. These functions extract or generate amino acid or nucleotide sequences and prepare them for deep learning tasks through [Keras3](https://tensorflow.rstudio.com/guides/keras/basics). **immApex** is the underlying structure for the BCR models in [Ibex](https://github.com/BorchLab/Ibex) and TCR models in [Trex](https://github.com/BorchLab/Trex). It should be noted that the tools here are created for immune receptor sequences; they will work more generally for nucleotide or amino acid sequences. The package itself supports AIRR, Adaptive, and 10x formats and interacts with the **scRepertoire** R package.

More information is available at the [immApex GitHub Repo](https://github.com/BorchLab/immApex).

## Loading Libraries

```{r}
suppressMessages(library(immApex))
suppressMessages(library(ggplot2))
suppressMessages(library(viridis))
suppressMessages(library(magrittr))
suppressMessages(library(igraph))
suppressMessages(library(tidygraph))
suppressMessages(library(ggraph))
```

# Acquiring and Preparing Repertoire Data

## getIMGT

Depending on the sequencing technology and the version, we might want to expand the length of our sequence embedding approach. The first step in the process is pulling the reference sequences from the ImMunoGeneTics (IMGT) system using ```getIMGT()```. More information for IMGT can be found at [imgt.org](https://www.imgt.org/). Data from IMGT is under a CC BY-NC-ND 4.0 license. Please be aware that attribution is required for usage and should not be used to create commercial or derivative work. 

Parameters for ```getIMGT()```

* **species** One or two word designation of species. Currently supporting: "human", "mouse", "rat", "rabbit", "rhesus monkey", "sheep", "pig", "platypus", "alpaca", "dog", "chicken", and "ferret"
* **chain** Sequence chain to access
* **frame** Designation for "all", "inframe" or "inframe+gap"
* **region** Sequence gene loci to access
* **sequence.type** Type of sequence - "aa" for amino acid or "nt" for nucleotide

Here, we will use the ```getIMGT()``` function to get the amino acid sequences for the TRBV region to get all the sequences by V gene allele.

```{r tidy = FALSE}
TRBV_aa <- getIMGT(species = "human",
                   chain = "TRB",
                   frame = "inframe",
                   region = "v",
                   sequence.type = "aa") 

TRBV_aa[[1]][1]
```

## formatGenes

Immune receptor nomenclature can be highly variable across sequencing platforms. When preparing data for models, we can use ```formatGenes()``` to universalize the gene formats into IMGT nomenclature.

Parameters for ```formatGenes()```

* **input.data** Data frame of sequencing data or scRepertoire outputs
* **region** Sequence gene loci to access - 'v', 'd', 'j', 'c' or a combination using c('v', 'd', 'j')
* **technology** The sequencing technology employed - 'TenX', "Adaptive', or 'AIRR'
* **species** One or two word designation of species. Currently supporting: "human", "mouse", "rat", "rabbit", "rhesus monkey", "sheep", "pig", "platypus", "alpaca", "dog", "chicken", and "ferret"
* **simplify.format** If applicable, remove the allelic designation (TRUE) or retain all information (FALSE)

Here, we will use the built-in example from Adaptive Biotechnologies and reformat and simplify the **v** region. ```formatGenes()``` will add 2 columns to the end of the data frame per region selected - 1) **v_IMGT** will be the formatted gene calls and 2) **v_IMGT.check** is a binary for if the formatted region appears in the IMGT database. In the example below, "TRBV2-1" is not recognized as a designation within IMGT.

```{r tidy = FALSE}
data("immapex_example.data")
Adaptive_example <- formatGenes(immapex_example.data[["Adaptive"]],
                                region = "v",
                                technology = "Adaptive", 
                                simplify.format = TRUE) 

head(Adaptive_example[,c("aminoAcid","vGeneName", "v_IMGT", "v_IMGT.check")])
```

## getIMGT

Depending on the sequencing technology and the version, we might want to expand the length of our sequence embedding approach. The first step in the process is pulling the reference sequences from the ImMunoGeneTics (IMGT) system using ```getIMGT()```. More information for IMGT can be found at [imgt.org](https://www.imgt.org/). Data from IMGT is under a CC BY-NC-ND 4.0 license. Please be aware that attribution is required for usage and should not be used to create commercial or derivative work. 

Parameters for ```getIMGT()```

* **species** One or two word designation of species. Currently supporting: "human", "mouse", "rat", "rabbit", "rhesus monkey", "sheep", "pig", "platypus", "alpaca", "dog", "chicken", and "ferret"
* **chain** Sequence chain to access
* **frame** Designation for "all", "inframe" or "inframe+gap"
* **region** Sequence gene loci to access
* **sequence.type** Type of sequence - "aa" for amino acid or "nt" for nucleotide

Here, we will use the ```getIMGT()``` function to get the amino acid sequences for the TRBV region to get all the sequences by V gene allele.

```{r tidy = FALSE}
TRBV_aa <- getIMGT(species = "human",
                   chain = "TRB",
                   frame = "inframe",
                   region = "v",
                   sequence.type = "aa") 

TRBV_aa[[1]][1]
```

## inferCDR

We can now use ```inferCDR()``` to add additional sequence elements to our example data using the outputs of ```formatGenes()``` and ```getIMGT()```. Here, we will use the function to isolate the complementarity-determining regions (CDR) 1 and 2. If the gene nomenclature does not match the IMGT the result will be NA for the given sequences. Likewise, if the IMGT nomenclature has been simplified, the first allelic match will be used for sequence extraction. 

Parameters for ```inferCDR```

* **input.data** Data frame of sequencing data or output from formatGenes().
* **reference** IMGT sequences from ```getIMGT()```
* **technology** The sequencing technology employed - 'TenX', "Adaptive', or 'AIRR',
* **sequence.type** Type of sequence - "aa" for amino acid or "nt" for nucleotide
* **sequences** The specific regions of the CDR loop to get from the data.

```{r tidy = FALSE}
Adaptive_example <- inferCDR(Adaptive_example,
                             chain = "TRB", 
                             reference = TRBV_aa,
                             technology = "Adaptive", 
                             sequence.type = "aa",
                             sequences = c("CDR1", "CDR2"))

Adaptive_example[200:210,c("CDR1_IMGT", "CDR2_IMGT")]
```

# Generating and Augmenting Sequence Sets

## generateSequences

Generating synthetic sequences is a quick way to start testing the model code. ```generateSequences()``` can also generate realistic noise for generative adversarial networks.

Parameters for ```generateSequences()```

* **prefix.motif** Add a defined sequence to the start of the generated sequences.
* **suffix.motif** Add a defined sequence to the end of the generated sequences
* **number.of.sequences** Number of sequences to generate
* **min.length** Minimum length of the final sequence (will be adjusted if incongruent with prefix.motif/suffix.motif)
* **max.length** Maximum length of the final sequence
* **sequence.dictionary** The letters to use in sequence generation (default are all amino acids)

```{r tidy = FALSE}
sequences <- generateSequences(prefix.motif = "CAS",
                               suffix.motif = "YF",
                               number.of.sequences = 1000,
                               min.length = 8,
                               max.length = 16)
sequences <- unique(sequences)
head(sequences)
```

If we want to generate nucleotide sequences instead of amino acids, we must to change the **sequence.dictionary**.

```{r tidy = FALSE}
nucleotide.sequences <- generateSequences(number.of.sequences = 1000,
                                          min.length = 8,
                                          max.length = 16, 
                                          sequence.dictionary = c("A", "C", "T", "G"))
head(nucleotide.sequences)
```

## mutateSequences

A common approach is to mutate sequences randomly or at specific intervals. This can be particularly helpful if we have fewer sequences or want to test a model for accuracy given new, altered sequences. ```mutateSequences()``` allows us to tune the type of mutation, where along the sequences to introduce the mutation and the overall number of mutations.

Parameters for ```mutateSequences()```

* **input.sequences** The amino acid or nucleotide sequences to use
* **number.of.sequences** The number of mutated sequences to return per input.sequence
* **mutation.rate** The rate of mutations introduced into sequences
* **position.start** The starting position to mutate along the sequence. Default NULL will start the random mutations at position 1.
* **position.end** The ending position to mutate along the sequence. Default NULL will end the random mutations at the last position.
* **sequence.dictionary** The letters to use in sequence mutation (default are all amino acids)

```{r tidy = FALSE}
mutated.sequences <- mutateSequences(sequences, 
                                     number.of.sequences = 1,
                                     position.start = 3,                                  
                                     position.end = 8)
head(sequences)
head(mutated.sequences)
```

# Feature Engineering from Repertoires

Beyond encoding single sequences, immApex provides functions to calculate summary statistics and features across a collection of sequences, such as a clonal repertoire.

## calculateFrequency

This function calculates the relative frequency of each residue at each position across a set of sequences.

Parameters for ```calculateFrequency()```

* **input.sequences**: A character vector of sequences.
* **max.length**: The length to align sequences to by padding/trimming.
* **tidy**: If TRUE, returns a long-format data frame suitable for ggplot2.

```{r tidy = FALSE}
freq.matrix <- calculateFrequency(sequences, 
                                  max.length = 20)
head(freq.matrix[, 1:10])
```

# calculateEntropy

This function measures the diversity (or entropy) at each position in a set of aligned sequences. It can use several common diversity metrics.

Parameters for `calculateEntropy()`

* **method**: The diversity metric to use: "shannon", "inv.simpson", "gini.simpson", "norm.entropy", "pielou", or Hill numbers ("hill0", "hill1", "hill2").

```{r tidy = FALSE}
shannon.entropy <- calculateEntropy(sequences, 
                                    method = "shannon")
head(shannon.entropy)
```

## calculateProperty

This function computes position-wise summary statistics for amino acid properties. For each position, it calculates a metric (like the mean) of a specific physicochemical property for all residues found at that position.

Parameters for `calculateProperty()`

* **property.set**: The amino acid property scale to use (e.g., "Atchley").
* **summary.fun**: The summary statistic to apply ("mean", "median", "sum" etc.).

```{r tidy = FALSE}
# Calculate the mean of Atchley factors at each position
atchley.profile <- calculateProperty(sequences, 
                                     property.set = "Atchley", 
                                     summary.fun = "mean")
head(atchley.profile[, 1:6])
```

## calculateGeneUsage

This function quantifies the usage of gene loci (e.g., V and J genes) within a repertoire.

Parameters for `calculateGeneUsage()`
* **loci**: A character vector of one or two column names corresponding to the gene loci.
* **summary**: The output format: "proportion" (default), "count", or "percent".

```{r tidy = FALSE}
# Using the built-in AIRR data
data_airr <- immapex_example.data[["AIRR"]]

# Calculate paired V-J gene usage as percentages
vj_usage <- calculateGeneUsage(data_airr, 
                               loci = c("v_call", "j_call"),
                               summary = "percent")
vj_usage[1:5, 1:5]
```

## calculateMotif

This function rapidly finds and counts contiguous (or gapped) amino acid or nucleotide motifs of specified lengths across a set of sequences.

Parameters for `calculateMotif()`

* **motif.lengths**: An integer vector of motif sizes to search for.
* **min.depth**: The minimum number of times a motif must appear to be included in the output.
* **discontinuous**: If TRUE, also finds motifs with a single gap (e.g., "C.S").

```{r tidy = FALSE}
motif.counts <- calculateMotif(sequences, 
                               motif.lengths = 3, 
                               min.depth = 5)
head(motif.counts)
```

## probabilityMatrix

This function calculates a positional probability matrix (PPM) for a group of sequences. This can be used to represent the sequence profile of an antigen-specific repertoire or to generate a sequence logo.

```{r tidy = FALSE}
ppm.matrix <- probabilityMatrix(sequences)
head(ppm.matrix)
```

The PPM can also be converted to a positional weight matrix (PWM) using a log-likelihood ratio.

```{r tidy = FALSE}
set.seed(42)
# Generate a sample background frequency
back.freq <- sample(1:1000, 20)
names(back.freq) <- amino.acids
back.freq <- back.freq/sum(back.freq)

pwm.matrix <- probabilityMatrix(sequences,
                                max.length = 20,
                                convert.PWM = TRUE,
                                background.frequencies = back.freq)
head(pwm.matrix)
```

## adjacencyMatrix and buildNetwork

These functions help analyze relationships between residues or whole sequences.

`adjacencyMatrix()` summarizes transitions between adjacent residues, creating a matrix of co-occurrence counts.

```{r tidy = FALSE}
adj.matrix <- adjacencyMatrix(sequences, 
                              normalize = FALSE)
adj.matrix[1:10, 1:10]
```

`buildNetwork()` constructs a similarity network where nodes are sequences and edges connect sequences with an edit distance below a given threshold.

```{r tidy = FALSE}
# Building an edge list
g1 <- buildNetwork(data.frame(sequences = sequences),
                   seq_col = "sequences",
                   threshold = 2)

# Forming network
graph <- graph_from_edgelist(as.matrix(g1[,1:2]))
E(graph)$weight <- g1[,3]

# Remove isolated nodes for clearer visualization
graph <- delete_vertices(graph, which(degree(graph) == 0))

# Convert to tidygraph for use with ggraph
g_tidy <- as_tbl_graph(graph)

# Plot the network
ggraph(g_tidy, layout = "fr") + 
  geom_edge_link(aes(width = weight), color = "black", alpha = 0.5) + 
  geom_node_point(aes(size = degree(g_tidy, mode = "all")),
                  fill = "steelblue", color= "black", shape = 21) +
  theme_void() + 
  scale_edge_width(range = c(0.1, 0.5)) + 
  labs(size = "Degree") +
  theme(legend.position = "right")
```

# Encoding Sequences for Model Input

## onehotEncoder

One hot encoding of amino acid or nucleotide sequences is a common method for transforming sequences into numeric matrices or arrays compatible with downstream applications.

Parameters for ```onehotEncoder()```

* **input.sequences** The amino acid or nucleotide sequences to use
* **max.length** Additional length to pad, NULL will pad sequences to the max length of input.sequences
* **sequence.dictionary** The letters to use in encoding (default are all amino acids)

```onehotEncoder()``` will return a list with the following elements:
* `cube` - Numeric array with dimensions  `c(depth, max.length, length(sequences))`.Depth = *sequence.dictionary size* in `"onehot"` mode, or *number of property scales* in `"property"` mode.
* `flattened` - Numeric matrix, `length(sequences)` rows by `depth × max.length` columns.
* Additional list elements will depend on the call but include `summary`, `sequence.dictionary`,  `padding.symbol`, and `threads`.


```{r tidy = FALSE}
encoded.object <- onehotEncoder(input.sequences =  c(sequences, mutated.sequences))
head(encoded.object)
```

## propertyEncoder

An alternative to one hot encoding is transforming the sequences into an array/matrix of numerical values using amino acid properties. 

These properties are largely based on dimensional reduction strategies, but it is essential to know the assumptions for each approach (links to original work below). **Important to note: ** this encoding strategy is specific for amino acids. 

**property.set**  

* atchleyFactors - [citation](https://pubmed.ncbi.nlm.nih.gov/15851683/)
* crucianiProperties - [citation](https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/abs/10.1002/cem.856)
* FASGAI - [citation](https://pubmed.ncbi.nlm.nih.gov/18318694/)
* kideraFactors - [citation](https://link.springer.com/article/10.1007/BF01025492)
* MSWHIM - [citation](https://pubs.acs.org/doi/10.1021/ci980211b)
* ProtFP - [citation](https://pubmed.ncbi.nlm.nih.gov/24059694/)
* stScales - [citation](https://pubmed.ncbi.nlm.nih.gov/19373543/)
* tScales - [citation](https://www.sciencedirect.com/science/article/abs/pii/S0022286006006314)
* VHSE - [citation](https://pubmed.ncbi.nlm.nih.gov/15895431/)
* zScales - [citation](https://pubmed.ncbi.nlm.nih.gov/9651153/)


```{r tidy = FALSE}
property.encoded <- propertyEncoder(input.sequences =  c(sequences, mutated.sequences), 
                                    property.set = "FASGAI")

head(property.encoded[[2]])
```

## geometricEncoder

One approach to encoding amino acid sequences is geometric isometry, such as [GIANA](https://pubmed.ncbi.nlm.nih.gov/34349111/).

Parameters for ```geometricEncoder()```

* **method** Select the following substitution matrices: "BLOSUM45", "BLOSUM50", "BLOSUM62", "BLOSUM80", "BLOSUM100", "PAM30", "PAM40", "PAM70", "PAM120", or "PAM250"
* **theta** The angle in which to create the rotation matrix

```{r tidy = FALSE}
geometric.matrix <- geometricEncoder(sequences, 
                                     method = "BLOSUM62",
                                     theta = pi/3)
head(geometric.matrix)
```

## tokenizeSequences

Another approach to transforming a sequence into numerical values is tokenizing it into numbers. This is a common approach for recurrent neural networks where one letter corresponds to a single integer. In addition, we can add start and stop tokens to our original sequences to differentiate between the beginning and end of the sequences. 

Parameters for ```tokenizeSequences()```

* **add.startstop** Add start and stop tokens to the sequence
* **start.token** The character to use for the start token
* **stop.token** The character to use for the stop token
* **max.length** Additional length to pad, NULL will pad sequences to the max length of input.sequences
* **convert.to.matrix** Return a matrix (**TRUE**) or a vector (**FALSE**)

```{r tidy = FALSE}
token.matrix <- tokenizeSequences(input.sequences =  c(sequences, mutated.sequences), 
                                  add.startstop = TRUE,
                                  start.token = "!",
                                  stop.token = "^", 
                                  convert.to.matrix = TRUE)
head(token.matrix[,1:18])
```

## sequenceDecoder

We have a function called ```sequenceDecoder()``` that extracts sequences from one-hot or property-encoded matrices or arrays. This function can be applied to any generative approach to sequence generation.

Parameters for ```sequenceDecoder()```

* **sequence.matrix** The encoded sequences to decode in an array or matrix
* **mode** The encoding mode used for decoding: `"onehot"` or `"property"`
* **property.set** For `mode = "property"`, a character vector of property names (e.g., `"Atchley"`) that were used for the original encoding.
* **call.threshold** The relative strictness of sequence calling with higher values being more stringent

```{r tidy = FALSE}
property.matrix <- propertyEncoder(input.sequences =  c(sequences, mutated.sequences), 
                                   property.set = "FASGAI")

property.sequences <- sequenceDecoder(property.matrix[[1]],
                                      mode = "property",
                                      property.set  = "FASGAI",
                                      call.threshold = 1)
head(sequences)
head(property.sequences)
```

A similar approach can be applied when using matrices or arrays derived from one-hot encoding: 

```{r tidy=FALSE}
sequence.matrix <- onehotEncoder(input.sequences =  c(sequences, mutated.sequences))

OHE.sequences <- sequenceDecoder(sequence.matrix,
                                 mode= "onehot")

head(OHE.sequences)
```

# Training a Model

## Autoencoder

For the vignette - we will use an autoencoder for sequence embedding. The code below is based on the [Trex](https://github.com/BorchLab/Trex) R package. The overall structure of the autoencoder is the same. However, some of the parameters are modified for the sake of the vignette. We will use the **sequence.matrix** we generated above from the ```onehotEncoder()```.

The steps to train the model include: 

1. Subsetting sequences
2. Defining parameters for the model
3. Forming the autoencoder structure - encoder and decoder
4. Fitting the model

```{r tidy = FALSE}
#Sampling to make Training/Validation Data Cohorts
set.seed(42)
num_sequences <- nrow(sequence.matrix[[2]])
indices <- 1:num_sequences
train_indices <- sample(indices, size = floor(0.8 * num_sequences))
val_indices <- setdiff(indices, train_indices)
    
x_train <- sequence.matrix[[2]][train_indices,]
x_val <- sequence.matrix[[2]][val_indices,]
   
# Parameters
input_shape <- dim(x_train)[2]
epochs <- 20
batch_size <- 128
encoding_dim <- 40 
hidden_dim1 <- 256 # Hidden layer 1 size
hidden_dim2 <- 128  # Hidden layer 2 size
    
es <- callback_early_stopping(monitor = "val_loss",
                              min_delta = 0,
                              patience = 4,
                              verbose = 1,
                              mode = "min")
                    
# Define the Model
input_seq <- layer_input(shape = c(input_shape))
        
# Encoder Layers
encoded <- input_seq %>%
          layer_dense(units = hidden_dim1, name = "e.1") %>%
          layer_batch_normalization(name = "bn.1") %>%
          layer_activation('leaky_relu', name = "act.1") %>%
          layer_dense(units = hidden_dim2, name = "e.2") %>%
          layer_batch_normalization(name = "bn.2") %>%
          layer_activation('leaky_relu', name = "act.2") %>%
          layer_dense(units = encoding_dim, activation = 'selu', name = "latent")
                
# Decoder Layers
decoded <- encoded %>%
          layer_dense(units = hidden_dim2, name = "d.2") %>%
          layer_batch_normalization(name = "bn.3") %>%
          layer_activation('leaky_relu', name = "act.3") %>%
          layer_dense(units = hidden_dim1, name = "d.1") %>%
          layer_batch_normalization(name = "bn.4") %>%
          layer_activation('leaky_relu', name = "act.4") %>%
          layer_dense(units = input_shape, activation = 'sigmoid')
      
# Autoencoder Model
autoencoder <- keras_model(input_seq, decoded)
autoencoder %>% keras3::compile(optimizer = optimizer_adam(learning_rate = 0.0001),
                                   loss = "mse")
      
# Train the model
history <- autoencoder %>% fit(x = x_train,
                               y = x_train,
                               validation_data = list(x_val, x_val),
                               epochs = epochs,
                               batch_size = batch_size,
                               shuffle = TRUE,
                               callbacks = es, 
                               verbose = 0)

plot(history) + 
  scale_color_viridis(option = "B", discrete = TRUE) + 
  scale_fill_manual(values = c("black","black")) + 
  theme_classic()
```

## Classifier

We can also build classifiers directly using deep or shallow neural networks. Building deep classifiers requires more data than classical machine learning methods, like random forests, so the vignette may not be ideal.

The first step is to generate distinct types of sequences using ```generateSequences()``` and ```onehotEncoder()``` to prepare the data for the model.

```{r tidy = FALSE}
class1.sequences <- generateSequences(prefix.motif = "CAS",
                                      suffix.motif = "YF",
                                      number.of.sequences = 10000,
                                      min.length = 8,
                                      max.length = 16)

class2.sequences <- generateSequences(prefix.motif = "CASF",
                                      suffix.motif = "YF",
                                      number.of.sequences = 10000,
                                      min.length = 8,
                                      max.length = 16)

labels <- as.numeric(c(rep(0, 10000), rep(1, 10000)))

classifier.matrix <- onehotEncoder(input.sequences =  c(class1.sequences, class2.sequences))
```

Next, we will define and train the Keras3 classifier model using artificial sequences. We will use a simple convolutional neural network with 2 layers and then a single neuron that will classify the sequences into class 1 or class 2 (here, the labels are 0 and 1).

```{r tidy = FALSE}
#Input shape will be 1D as we are using a matrix
input.shape <- dim(classifier.matrix[[2]])[2]

#Simple model structure
classifier.model <- keras_model_sequential() %>% 
                        layer_dense(units = 128, activation = "relu", 
                                    input_shape = c(input.shape)) %>%
                        layer_dense(units = 32, activation = "relu") %>%
                        layer_dense(units = 1, activation = "sigmoid")

classifier.model %>% compile(
        optimizer = optimizer_adam(learning_rate = 0.00001),
        loss = "binary_crossentropy",
        metrics = c("accuracy")
)

#Separating data and labels
set.seed(42)
val_indices <- sample(nrow(classifier.matrix[[2]]), 10000*0.2)
x_val <- classifier.matrix[[2]][val_indices,]
x_train <- classifier.matrix[[2]][-val_indices,]

val_labels <- labels[val_indices]
train_labels <- labels[-val_indices]

#Training the classifier.model
history <- classifier.model %>% fit(x_train, 
                                    train_labels, 
                                    epochs = 20, 
                                    batch_size = 32, 
                                    validation_data = list(x_val, val_labels),
                                    verbose = 0
)

plot(history) + 
  scale_color_viridis(option = "B", discrete = TRUE) + 
  scale_fill_manual(values = c("black","black")) + 
  theme_classic()
```

```{r, include=FALSE}
keras3::clear_session()
```

Here, we can achieve a validation accuracy of 98.25%, which is impressive. But to contextualize, we used ```generateSequences()``` and distinct motifs - "CAS" vs "CASF" to create our 2 classes of sequences. Using sequences from experimental data will likely result in lower accuracy or require greater model complexity.

***

# Conclusion

This has been a general overview of the capabilities of **immApex** for processing immune receptor sequences and making deep learning models. If you have any questions, comments, or suggestions, feel free to visit the [GitHub repository](https://github.com/BorchLab/immApex).

## Session Info

```{r}
sessionInfo()
```