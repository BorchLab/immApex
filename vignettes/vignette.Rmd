---
title: Welcome to Apex
author: 
- name: Nick Borcherding
  email: ncborch@gmail.com
  affiliation: Washington University in St. Louis, School of Medicine, St. Louis, MO, USA
date: "April 26, 2024"
output:
  BiocStyle::html_document:
    toc_float: true
package: Trex
vignette: >
  %\VignetteEngine{knitr::knitr}
  %\VignetteIndexEntry{Running Apex}
  %\usepackage[UTF-8]{inputenc}
---

```{r, echo=FALSE, results="hide", message=FALSE}
knitr::opts_chunk$set(error=FALSE, message=FALSE, warning=FALSE)
library(BiocStyle)
```

# Introduction

Apex is meant to serve as an API for deep learning models based on immune receptor sequencing. These collection of functions serve to extract or generate amino acid sequences and then preparing the sequences for deep learning tasks through Keras. Although the tools here are created for immune receptor sequences, they will work more generally for nucleotide or amino acid sequences of any kind. 

## Load Libraries

```{r}
suppressMessages(library(Apex))
suppressMessages(library(keras))
suppressMessages(library(scRepertoire))
```

****************

# Getting and Manipulating Sequences

## Generate.sequences

A quick way to get started with testing the model code would be to generate synthetic sequences. ```generate.sequences()``` can also be used to generate realistic noise for generative adversarial networks. 

Parameters for ```generate.sequences()```

* **prefix.motif** Add defined amino acid sequence to the start of the generated sequences.
* **suffix.motif** Add defined amino acid sequence to the end of the generated sequences
number.of.sequences Number of sequences to generate
* **min.length** Minimum length of the final sequence (will be adjusted if incongruent with prefix.motif/suffix.motif)
* **max.length** Maximum length of the final sequence
* **sequence.dictionary** The letters to use in sequence generation (default are all amino acids)

```{r}
sequences <- generate.sequences(prefix.motif = "CAS",
                                suffix.motif = "YF",
                                number.of.sequences = 1000,
                                min.length = 8,
                                max.length = 16)
head(sequences)
```

If instead of amino acides, we want to look at nucleotide sequences, we just need to change the **sequence.dictionary**.

```{r}
nucleotide.sequences <- generate.sequences(number.of.sequences = 1000,
                                           min.length = 8,
                                           max.length = 16, 
                                           sequence.dictionary = c("A", "C", "T", "G"))
head(nucleotide.sequences)
```

## Mutate.sequences

A common approach is to take sequences and mutate them randomly or at specific intervals. This can be particularly helpful if we have a low number of sequences or want to test a model for accuracy given new, altered sequences. ```mutate.sequences()``` allows us to tune the type of mutation, where along the sequences to introduce the mutation, and the overall number of mutations.

Parameters for ```mutate.sequences()```

* **input.sequences** The amino acid or nucleotide sequences to use
* **n.sequences** The number of mutated sequences to return per input.sequence
* **mutation.rate** The rate of mutations to introduce into sequences
* **position.start** The starting position to mutate along the sequence.Default NULL will start the random mutations at position 1.
* **position.end** The ending position to mutate along the sequence.Default NULL will end the random mutations at the last position.
* **sequence.dictionary** The letters to use in sequence mutation (default are all amino acids)

```{r}
mutated.sequences <- mutate.sequences(sequences, 
                                      n.sequence = 1,
                                      position.start = 3,                                  
                                      position.end = 8)
head(sequences)
head(mutated.sequences)
```

## Format.genes

Across sequencing platforms immune receptor nomenclature can be highly variable, preparing data for models, we can use ```format.genes()``` to universalize the gene formats into IMGT nomenclature. 

Parameters for ```format.genes()```
* **input.data** Data frame of sequencing data or scRepertoire outputs
* **region** Sequence gene loci to access - 'v', 'd', 'j', 'c' or a combination using c('v', 'd', 'j')
* **technology** The sequencing technology employed - 'TenX', "Adaptive', 'AIRR', or 'Omniscope'.
* **species** One or two word designation of species. Currently supporting: "human", "mouse", "rat", "rabbit", "rhesus monkey", "sheep", "pig", "platypus", "alpaca", "dog", "chicken", and "ferret"
* **simplify.format** If applicable, remove the allelic designation (TRUE) or retain all information (FALSE)

Here we will use the built-in example from Adaptive Biotechnologies and reformat and simplify the **v** region. ```format.genes()``` will add 2 columns to the end of the data frame per region selected - 1) **v_IMGT** will be the formatted gene calls and 2) **v_IMGT.check** is a binary for if the formatted region appears in the IMGT database. In the below example "TRBV2-1" is not a recognized designation within IMGT.


```{r}
Adaptive_example <- format.genes(Apex_example.data[["Adaptive"]],
                                 region = "v",
                                 technology = "Adaptive", 
                                 simplify.format = TRUE) 

head(Adaptive_example[,c("aminoAcid","vGeneName", "v_IMGT", "v_IMGT.check")])
```



# Encoders

## One.hot.encoder

One hot encoding of amino acid or nucleotide sequences is a common method to transform sequences into numeric matrices compatible with kera (or other workflows).

Parameters for ```one.hot.encoder()```

* **input.sequences** The amino acid or nucleotide sequences to use
* **max.length** Additional length to pad, NULL will pad sequences to the max length of input.sequences
* **convert.to.matrix** Return a matrix (**TRUE**) or a 3D array (**FALSE**)
* **sequence.dictionary** The letters to use in encoding (default are all amino acids + NA value)

```{r}
sequence.matrix <- one.hot.encoder(input.sequences =  c(sequences, mutated.sequences), 
                                   convert.to.matrix = TRUE)
head(sequence.matrix[,1:20])
```

## Property.encoder

An alternative to one hot encoding is to transform the sequences into an array/matrix of numerical values using amino acid properties. 

These properties are largely based on dimensional reduction strategies, but it is important to know the assumptions for each approach (links to original work below). **Important to note** this encoding strategy is specific for amino acids. 

**method.to.use**  

* atchleyFactors - \href{https://pubmed.ncbi.nlm.nih.gov/15851683/}{citation}).
* crucianiProperties - \href{https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/abs/10.1002/cem.856}{citation}).
* FASGAI - \href{https://pubmed.ncbi.nlm.nih.gov/18318694/}).
* kideraFactors - \href{https://link.springer.com/article/10.1007/BF01025492}{citation}).
* MSWHIM - \href{https://pubs.acs.org/doi/10.1021/ci980211b}{citation}).
* ProtFP - \href{https://pubmed.ncbi.nlm.nih.gov/24059694/}{citation}).
* stScales - \href{https://pubmed.ncbi.nlm.nih.gov/19373543/}{citation}).
* tScales - \href{https://www.sciencedirect.com/science/article/abs/pii/S0022286006006314}{citation}).
* VHSE - \href{https://pubmed.ncbi.nlm.nih.gov/15895431/}{citation}).
* zScales - \href{https://pubmed.ncbi.nlm.nih.gov/9651153/}{citation}).


```{r}
property.matrix <- property.encoder(input.sequences =  c(sequences, mutated.sequences), 
                                    method.to.use = "FASGAI",
                                    convert.to.matrix = TRUE)

head(property.matrix[,1:20])
```

```property.encoder()``` also allows for us to use multiple approaches at the same time by setting **method.to.use** as a vector.

```{r}
mulit.property.matrix <- property.encoder(input.sequences =  c(sequences, mutated.sequences), 
                                          method.to.use = c("atchleyFactors", "kideraFactors"),
                                          convert.to.matrix = TRUE)

head(mulit.property.matrix[,1:20])
```

If instead, we would like to get the a set of summarized values across all amino acid residues for a given **method.to.use**, we can use **summary.function** and select "median", "mean", "sum", variance ("vars"), or Median Absolute Deviation ("mads"). 


```{r}
median.property.matrix <- property.encoder(input.sequences =  c(sequences, mutated.sequences), 
                                          method.to.use = "crucianiProperties",
                                          summary.function = "median")
```

## Geometric.encoder

Parameters for ```geometric.encoder()```

* **method.to.use** Select the following substitution matrices: "BLOSUM45", "BLOSUM50", "BLOSUM62", "BLOSUM80", "BLOSUM100", "PAM30", "PAM40", "PAM70", "PAM120", or "PAM250"

* **theta** The angle in which to create the rotataion matrix

```{r}
geometric.matrix <- geometric.encoder(sequences, 
                                      method.to.use = "BLOSUM62",
                                      theta = pi/3)
head(geometric.matrix)
```

## Tokenize.sequences

Another approach to transforming a sequence into numerical values is tokenizing it into numbers. This is a common approach for recurrent neural networks where one letter correspond to a single integer. In addition, we can add start and stop tokens to our original sequences in order to differentiate between the beginning and end of the sequences. 

Parameters for ```tokenize.sequences()```

* **add.startstop** Add start and stop tokens to the sequence
* **start.token** The character to use for the start token
* **stop.token** The character to use for the stop token
* **max.length** Additional length to pad, NULL will pad sequences to the max length of input.sequences
* **convert.to.matrix** Return a matrix (**TRUE**) or a vector (**FALSE**)


```{r}
token.matrix <- tokenize.sequences(input.sequences =  c(sequences, mutated.sequences), 
                                   add.startstop = TRUE,
                                   start.token = "!",
                                   stop.token = "^", 
                                   convert.to.matrix = TRUE)
head(token.matrix[,1:18])
```

## PPM.sequences

Another method for encoding a group of sequences is to calculate the positional probability of sequences using ```PPM.sequences()```. This could be used to represent a collection of antigen-specific sequences or even work on embedding a total repertoire.  

```{r}
ppm.matrix <- PPM.sequences(sequences)
head(ppm.matrix)
```
In addition, ```PPM.sequences()``` can be used to convert the positional probability matrix into a positional weight matrix using log likelihood using the argument **convert.PWM** = TRUE. We can provide a set of background frequencies for the amino acids with **background.frequencies** or leave this blank to assume a uniform distribution for all amino acids. Here we are going to use a example background. 

```{r}
set.seed(42)
back.freq <- sample(1:1000, 20)
back.freq <- back.freq/sum(back.freq)

pwm.matrix <- PPM.sequences(sequences,
                            max.length = 20,
                            convert.PWM = TRUE,
                            background.frequencies = back.freq)
head(pwm.matrix)
```


## Positional.encoder

```{r}
positional.matrix <- positional.encoder(number.of.sequences = length(c(sequences, mutated.sequences)),
                                        latent.dims = 40)
```

# Training a Model

## Autoencoder

For the purposes of the vignette - we will use a autoencoder for sequence embedding. The code below is based on the [Trex](https://github.com/ncborcherding/Trex) R package. The overall structure of the autoencoder is the same, however, some of the parameters are modified for the sake of the vignette.We will use the **sequence.matrix** we generated above from the ```one.hot.encoder()```.

The steps to train the model include: 

1. Subsetting sequences
2. Defining parameters for the model
3. Forming the autoencoder structure - encoder and decoder
4. Fitting the model

```{r}
#Sampling to make Training/Valid Data
set.seed(42)
num_sequences <- nrow(sequence.matrix)
indices <- 1:num_sequences
train_indices <- sample(indices, size = floor(0.8 * num_sequences))
val_indices <- setdiff(indices, train_indices)
    
x_train <- sequence.matrix[train_indices,]
x_val <- sequence.matrix[val_indices,]
   
# Parameters
input_shape <- dim(x_train)[2]
epochs <- 20
batch_size <- 128
encoding_dim <- 40 
hidden_dim1 <- 256 # Hidden layer 1 size
hidden_dim2 <- 128  # Hidden layer 2 size
    
es = callback_early_stopping(monitor = "val_loss",
                             min_delta = 0,
                             patience = 4,
                             verbose = 1,
                             mode = "min")
                    
# Define the Model
input_seq <- layer_input(shape = c(input_shape))
        
# Encoder Layers
encoded <- input_seq %>%
          layer_dense(units = hidden_dim1, name = "e.1") %>%
          layer_batch_normalization(name = "bn.1") %>%
          layer_activation('leaky_relu', name = "act.1") %>%
          layer_dense(units = hidden_dim2, name = "e.2") %>%
          layer_batch_normalization(name = "bn.2") %>%
          layer_activation('leaky_relu', name = "act.2") %>%
          layer_dense(units = encoding_dim, activation = 'selu', name = "latent")
                
# Decoder Layers
decoded <- encoded %>%
          layer_dense(units = hidden_dim2, name = "d.2") %>%
          layer_batch_normalization(name = "bn.3") %>%
          layer_activation('leaky_relu', name = "act.3") %>%
          layer_dense(units = hidden_dim1, name = "d.1") %>%
          layer_batch_normalization(name = "bn.4") %>%
          layer_activation('leaky_relu', name = "act.4") %>%
          layer_dense(units = input_shape, activation = 'sigmoid')
      
# Autoencoder Model
autoencoder <- keras_model(input_seq, decoded)
autoencoder %>% keras::compile(optimizer = optimizer_adam(learning_rate = 0.0001),
                                   loss = "mse")
      
# Train the model
history <- autoencoder %>% fit(x = x_train,
                               y = x_train,
                               validation_data = list(x_val, x_val),
                               epochs = epochs,
                               batch_size = batch_size,
                               shuffle = TRUE,
                               callbacks = es)

plot(history)
```

## Classifier



***
# Conclusion

This has been a general overview of the capabilities of Apex for processing immune receptor sequences and making deep learning models. If you have any questions, comments, or suggestions, feel free to visit the [GitHub repository](https://github.com/ncborcherding/Apex).

## Session Info

```{r}
sessionInfo()
```
